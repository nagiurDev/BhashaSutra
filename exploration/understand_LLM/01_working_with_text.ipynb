{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Working with Text",
   "id": "3bbcc26a00698205"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T14:53:32.520976Z",
     "start_time": "2024-10-04T14:53:32.492553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from importlib.metadata import version\n",
    "import re\n",
    "\n",
    "print(\"Torch version:\", version(\"torch\"))\n",
    "print(\"Tiktoken version:\", version(\"tiktoken\"))"
   ],
   "id": "38f1c81039271765",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.3.0\n",
      "Tiktoken version: 0.8.0\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.1 Data Loading",
   "id": "ff54400fa8ac6373"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T14:53:34.967655Z",
     "start_time": "2024-10-04T14:53:34.952579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"../../data/raw/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "print(\"Length of raw text: \", len(raw_text))"
   ],
   "id": "843273d81836a2fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of raw text:  20479\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.2 Tokenization",
   "id": "8d1d23138fdad57"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T14:53:36.445465Z",
     "start_time": "2024-10-04T14:53:36.435736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text  = \"Hello, world. This, is a test.\"\n",
    "\n",
    "result = re.split(r\"\\W+\", text)\n",
    "print(\"Tokens: \", result)\n",
    "\n",
    "result = re.split(r'([,.]|\\s)', text)\n",
    "print(\"Tokens: \", result)"
   ],
   "id": "909a4785135b5b24",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:  ['Hello', 'world', 'This', 'is', 'a', 'test', '']\n",
      "Tokens:  ['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T14:53:37.266338Z",
     "start_time": "2024-10-04T14:53:37.248007Z"
    }
   },
   "cell_type": "code",
   "source": "[item for item in result if item.strip()]",
   "id": "1bb336966852ca94",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.2.1 Apply tokenization to the raw text",
   "id": "7e3f4e05851e9288"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T14:53:38.552787Z",
     "start_time": "2024-10-04T14:53:38.529311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "print(preprocessed[:10])"
   ],
   "id": "a45afe0b19fba902",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', ' ', 'HAD', ' ', 'always', ' ', 'thought', ' ', 'Jack', ' ']\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T14:53:39.232499Z",
     "start_time": "2024-10-04T14:53:39.224077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ],
   "id": "743f84b539f6821b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### The total number of tokens",
   "id": "92c4f9f85b5e7458"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T14:53:40.698129Z",
     "start_time": "2024-10-04T14:53:40.684294Z"
    }
   },
   "cell_type": "code",
   "source": "len(preprocessed)",
   "id": "86a73cc3ffccef0b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4690"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.3 Convert tokens into token IDs",
   "id": "9f22aa3857a147b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T14:53:42.067586Z",
     "start_time": "2024-10-04T14:53:42.059934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(\"Vocabulary size: \", vocab_size)"
   ],
   "id": "ff4c92ad1b44f06e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  1130\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T14:53:42.832771Z",
     "start_time": "2024-10-04T14:53:42.783571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
    "\n",
    "list(vocab.items())[:200]"
   ],
   "id": "12305085f5a11728",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('!', 0),\n",
       " ('\"', 1),\n",
       " (\"'\", 2),\n",
       " ('(', 3),\n",
       " (')', 4),\n",
       " (',', 5),\n",
       " ('--', 6),\n",
       " ('.', 7),\n",
       " (':', 8),\n",
       " (';', 9),\n",
       " ('?', 10),\n",
       " ('A', 11),\n",
       " ('Ah', 12),\n",
       " ('Among', 13),\n",
       " ('And', 14),\n",
       " ('Are', 15),\n",
       " ('Arrt', 16),\n",
       " ('As', 17),\n",
       " ('At', 18),\n",
       " ('Be', 19),\n",
       " ('Begin', 20),\n",
       " ('Burlington', 21),\n",
       " ('But', 22),\n",
       " ('By', 23),\n",
       " ('Carlo', 24),\n",
       " ('Chicago', 25),\n",
       " ('Claude', 26),\n",
       " ('Come', 27),\n",
       " ('Croft', 28),\n",
       " ('Destroyed', 29),\n",
       " ('Devonshire', 30),\n",
       " ('Don', 31),\n",
       " ('Dubarry', 32),\n",
       " ('Emperors', 33),\n",
       " ('Florence', 34),\n",
       " ('For', 35),\n",
       " ('Gallery', 36),\n",
       " ('Gideon', 37),\n",
       " ('Gisburn', 38),\n",
       " ('Gisburns', 39),\n",
       " ('Grafton', 40),\n",
       " ('Greek', 41),\n",
       " ('Grindle', 42),\n",
       " ('Grindles', 43),\n",
       " ('HAD', 44),\n",
       " ('Had', 45),\n",
       " ('Hang', 46),\n",
       " ('Has', 47),\n",
       " ('He', 48),\n",
       " ('Her', 49),\n",
       " ('Hermia', 50),\n",
       " ('His', 51),\n",
       " ('How', 52),\n",
       " ('I', 53),\n",
       " ('If', 54),\n",
       " ('In', 55),\n",
       " ('It', 56),\n",
       " ('Jack', 57),\n",
       " ('Jove', 58),\n",
       " ('Just', 59),\n",
       " ('Lord', 60),\n",
       " ('Made', 61),\n",
       " ('Miss', 62),\n",
       " ('Money', 63),\n",
       " ('Monte', 64),\n",
       " ('Moon-dancers', 65),\n",
       " ('Mr', 66),\n",
       " ('Mrs', 67),\n",
       " ('My', 68),\n",
       " ('Never', 69),\n",
       " ('No', 70),\n",
       " ('Now', 71),\n",
       " ('Nutley', 72),\n",
       " ('Of', 73),\n",
       " ('Oh', 74),\n",
       " ('On', 75),\n",
       " ('Once', 76),\n",
       " ('Only', 77),\n",
       " ('Or', 78),\n",
       " ('Perhaps', 79),\n",
       " ('Poor', 80),\n",
       " ('Professional', 81),\n",
       " ('Renaissance', 82),\n",
       " ('Rickham', 83),\n",
       " ('Riviera', 84),\n",
       " ('Rome', 85),\n",
       " ('Russian', 86),\n",
       " ('Sevres', 87),\n",
       " ('She', 88),\n",
       " ('Stroud', 89),\n",
       " ('Strouds', 90),\n",
       " ('Suddenly', 91),\n",
       " ('That', 92),\n",
       " ('The', 93),\n",
       " ('Then', 94),\n",
       " ('There', 95),\n",
       " ('They', 96),\n",
       " ('This', 97),\n",
       " ('Those', 98),\n",
       " ('Though', 99),\n",
       " ('Thwing', 100),\n",
       " ('Thwings', 101),\n",
       " ('To', 102),\n",
       " ('Usually', 103),\n",
       " ('Venetian', 104),\n",
       " ('Victor', 105),\n",
       " ('Was', 106),\n",
       " ('We', 107),\n",
       " ('Well', 108),\n",
       " ('What', 109),\n",
       " ('When', 110),\n",
       " ('Why', 111),\n",
       " ('Yes', 112),\n",
       " ('You', 113),\n",
       " ('_', 114),\n",
       " ('a', 115),\n",
       " ('abdication', 116),\n",
       " ('able', 117),\n",
       " ('about', 118),\n",
       " ('above', 119),\n",
       " ('abruptly', 120),\n",
       " ('absolute', 121),\n",
       " ('absorbed', 122),\n",
       " ('absurdity', 123),\n",
       " ('academic', 124),\n",
       " ('accuse', 125),\n",
       " ('accustomed', 126),\n",
       " ('across', 127),\n",
       " ('activity', 128),\n",
       " ('add', 129),\n",
       " ('added', 130),\n",
       " ('admirers', 131),\n",
       " ('adopted', 132),\n",
       " ('adulation', 133),\n",
       " ('advance', 134),\n",
       " ('aesthetic', 135),\n",
       " ('affect', 136),\n",
       " ('afraid', 137),\n",
       " ('after', 138),\n",
       " ('afterward', 139),\n",
       " ('again', 140),\n",
       " ('ago', 141),\n",
       " ('ah', 142),\n",
       " ('air', 143),\n",
       " ('alive', 144),\n",
       " ('all', 145),\n",
       " ('almost', 146),\n",
       " ('alone', 147),\n",
       " ('along', 148),\n",
       " ('always', 149),\n",
       " ('am', 150),\n",
       " ('amazement', 151),\n",
       " ('amid', 152),\n",
       " ('among', 153),\n",
       " ('amplest', 154),\n",
       " ('amusing', 155),\n",
       " ('an', 156),\n",
       " ('and', 157),\n",
       " ('another', 158),\n",
       " ('answer', 159),\n",
       " ('answered', 160),\n",
       " ('any', 161),\n",
       " ('anything', 162),\n",
       " ('anywhere', 163),\n",
       " ('apparent', 164),\n",
       " ('apparently', 165),\n",
       " ('appearance', 166),\n",
       " ('appeared', 167),\n",
       " ('appointed', 168),\n",
       " ('are', 169),\n",
       " ('arm', 170),\n",
       " ('arm-chair', 171),\n",
       " ('arm-chairs', 172),\n",
       " ('arms', 173),\n",
       " ('art', 174),\n",
       " ('articles', 175),\n",
       " ('artist', 176),\n",
       " ('as', 177),\n",
       " ('aside', 178),\n",
       " ('asked', 179),\n",
       " ('at', 180),\n",
       " ('atmosphere', 181),\n",
       " ('atom', 182),\n",
       " ('attack', 183),\n",
       " ('attention', 184),\n",
       " ('attitude', 185),\n",
       " ('audacities', 186),\n",
       " ('away', 187),\n",
       " ('awful', 188),\n",
       " ('axioms', 189),\n",
       " ('azaleas', 190),\n",
       " ('back', 191),\n",
       " ('background', 192),\n",
       " ('balance', 193),\n",
       " ('balancing', 194),\n",
       " ('balustraded', 195),\n",
       " ('basking', 196),\n",
       " ('bath-rooms', 197),\n",
       " ('be', 198),\n",
       " ('beaming', 199)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.3.1 Implement encode and decode \n",
    "- The `encode` function turns text into token IDs\n",
    "- The `decode` function turns token IDs back into text"
   ],
   "id": "b88056df07a22150"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T14:53:44.437269Z",
     "start_time": "2024-10-04T14:53:44.419541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_init = vocab\n",
    "        self.int_to_str = {v: k for k, v in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        \n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "        ids = [self.str_to_init[token] for token in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ],
   "id": "b8e9b47997ebe4b1",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T14:53:46.871386Z",
     "start_time": "2024-10-04T14:53:46.857120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"It's the last he painted, you know,\"\n",
    "        \"Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ],
   "id": "e917cb9278d84cda",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T14:53:48.048617Z",
     "start_time": "2024-10-04T14:53:48.036906Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.decode(ids)",
   "id": "d7e6ba9c50a345c1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\"\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.4 Adding special tokens",
   "id": "44415c81997ba3c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T14:53:49.852029Z",
     "start_time": "2024-10-04T14:53:49.841476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_words.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer, token in enumerate(all_words)}"
   ],
   "id": "72faf6823bef88b1",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T14:53:50.630361Z",
     "start_time": "2024-10-04T14:53:50.613888Z"
    }
   },
   "cell_type": "code",
   "source": "len(vocab.items())",
   "id": "f6953e4836711dd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T14:53:51.776868Z",
     "start_time": "2024-10-04T14:53:51.768442Z"
    }
   },
   "cell_type": "code",
   "source": "list(vocab.items())[-3:]",
   "id": "ea006534a706d83a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yourself', 1129), ('<|endoftext|>', 1130), ('<|unk|>', 1131)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T14:53:52.880731Z",
     "start_time": "2024-10-04T14:53:52.870093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_init = vocab\n",
    "        self.int_to_str = {v: k for k, v in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        \n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_init\n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "\n",
    "        ids = [self.str_to_init[token] for token in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ],
   "id": "77237ff47392dcf5",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T14:57:59.253552Z",
     "start_time": "2024-10-04T14:57:59.240032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text3 = \"Dhaka Gazipur  Amtoli\"\n",
    "\n",
    "text = \" <|endotext|> \".join((text1, text2, text3))\n",
    "print(text)"
   ],
   "id": "7cd77946fee91975",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endotext|> In the sunlit terraces of the palace. <|endotext|> Dhaka Gazipur  Amtoli\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T14:58:00.307077Z",
     "start_time": "2024-10-04T14:58:00.299542Z"
    }
   },
   "cell_type": "code",
   "source": "print(tokenizer.encode(text))",
   "id": "d5476de1be391ab8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 5, 355, 1126, 628, 975, 10, 1131, 55, 988, 956, 984, 722, 988, 1131, 7, 1131, 1131, 1131, 1131]\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T14:57:34.188136Z",
     "start_time": "2024-10-04T14:57:34.179533Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "5da9216e96aa0a3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cf4fad5660caabd1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
